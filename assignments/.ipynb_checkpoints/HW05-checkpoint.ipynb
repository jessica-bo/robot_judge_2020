{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Double ML with XGBoost\n",
    "(due October 27th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data (NLSY)\n",
    "import pandas as pd\n",
    "df = pd.read_stata('http://www.stata-press.com/data/r10/nlswork.dta')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Y = df['ln_wage']\n",
    "D = df['union']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill in X with all predictors that are not colliders\n",
    "X = df[['age', 'year', 'birth_yr', 'nev_mar', 'not_smsa', 'wks_ue','msp', 'idcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into sample A and sample B\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_A, X_B, Y_A, Y_B, D_A, D_B = train_test_split(X, Y, D, test_size=0.5)\n",
    "\n",
    "# Within each sample, make a validation set for xgboost early stopping\n",
    "X_A_train, X_A_eval, Y_A_train, Y_A_eval, D_A_train, D_A_eval = train_test_split(X_A, Y_A, D_A)\n",
    "X_B_train, X_B_eval, Y_B_train, Y_B_eval, D_B_train, D_B_eval = train_test_split(X_B, Y_B, D_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:28:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { early_stopping_rounds, eval_set } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:28:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { early_stopping_rounds, eval_set } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=2,\n",
       "             eval_set=[        age  year  birth_yr  nev_mar  not_smsa  wks_ue  msp  idcode\n",
       "22472  24.0    77        52      0.0       1.0     0.0  1.0    4103\n",
       "13559  40.0    83        42      0.0       0.0     0.0  1.0    2480\n",
       "22622  40.0    87        46      0.0       0.0     8.0  1.0    4129\n",
       "24574  43.0    88        45      0.0       1.0     1.0  0.0    4473\n",
       "24824  25.0    77        51      0.0       1.0    27.0  1.0    4515\n",
       "...     ...   ...       ...      ...       ...     ...  ....\n",
       "             gamma=0, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, objective='reg:squarederror', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1. In both samples, use xgboost regressor to predict log wages (outcome Y)\n",
    "# use early stopping.\n",
    "from xgboost import XGBRegressor\n",
    "xgbr_A_Y = XGBRegressor(eval_set=[X_A_eval, Y_A_eval], early_stopping_rounds=2)\n",
    "xgbr_A_Y.fit(X_A_train, Y_A_train)\n",
    "\n",
    "xgbr_B_Y = XGBRegressor(eval_set=[X_B_eval, Y_B_eval], early_stopping_rounds=2)\n",
    "xgbr_B_Y.fit(X_B_train, Y_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:28:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { early_stopping_rounds, eval_set } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:28:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { early_stopping_rounds, eval_set } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=2,\n",
       "              eval_set=[        age  year  birth_yr  nev_mar  not_smsa  wks_ue  msp  idcode\n",
       "22472  24.0    77        52      0.0       1.0     0.0  1.0    4103\n",
       "13559  40.0    83        42      0.0       0.0     0.0  1.0    2480\n",
       "22622  40.0    87        46      0.0       0.0     8.0  1.0    4129\n",
       "24574  43.0    88        45      0.0       1.0     1.0  0.0    4473\n",
       "24824  25.0    77        51      0.0       1.0    27.0  1.0    4515\n",
       "...     ...   ...       ...      ...       ...     ......\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2. In both samples, use xgboost classifier to predict union status (treatment D)\n",
    "# use early stopping.\n",
    "from xgboost import XGBClassifier\n",
    "xgbc_A_D = XGBClassifier(eval_set=[X_A_eval, D_A_eval], early_stopping_rounds=2)\n",
    "xgbc_A_D.fit(X_A_train, D_A_train)\n",
    "\n",
    "xgbc_B_D = XGBClassifier(eval_set=[X_B_eval, D_B_eval], early_stopping_rounds=2)\n",
    "xgbc_B_D.fit(X_B_train, D_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Cross-fitting: Form predictions in other sample.\n",
    "\n",
    "# predict wages in sample A using model trained in sample B:\n",
    "Y_hat_A = xgbr_B_Y.predict(X_A)\n",
    "\n",
    "# vice versa:\n",
    "Y_hat_B = xgbr_A_Y.predict(X_B)\n",
    "    \n",
    "# predict union status in sample A using model trained in sample B:\n",
    "D_hat_A = xgbc_B_D.predict(X_A)\n",
    "    \n",
    "# vice versa:\n",
    "D_hat_B = xgbc_A_D.predict(X_B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals for wages and union status.\n",
    "\n",
    "# residualized wages in samples A and B:\n",
    "Y_tilde_A = Y_A - Y_hat_A\n",
    "Y_tilde_B =  Y_B - Y_hat_B\n",
    "\n",
    "# residualized union status in samples A and B:\n",
    "D_tilde_A = D_A - D_hat_A\n",
    "D_tilde_B = D_B - D_hat_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\jessi\\anaconda3\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from statsmodels) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\jessi\\appdata\\roaming\\python\\python37\\site-packages (from statsmodels) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.21 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from statsmodels) (1.0.5)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\jessi\\anaconda3\\lib\\site-packages (from patsy>=0.5->statsmodels) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# as a shortcut, use statsmodels.\n",
    "# for projects (not homework), we would do this in stata.\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DML Coeff: union    0.126678\n",
      "dtype: float64\n",
      "DML S.E.: union    0.010298\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Run OLS regressions and produce DML estimate\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "model_A = sm.OLS(Y_tilde_A, D_tilde_A)\n",
    "result_A = model_A.fit(cov_type='HC1')\n",
    "beta_A = result_A.params\n",
    "se_A = result_A.bse\n",
    "\n",
    "model_B = sm.OLS(Y_tilde_B, D_tilde_B)\n",
    "result_B = model_B.fit(cov_type='HC1')\n",
    "beta_B = result_B.params\n",
    "se_B = result_B.bse\n",
    "\n",
    "beta = .5 * (beta_A + beta_B)\n",
    "se = .5 * (se_A + se_B)\n",
    "\n",
    "print('DML Coeff:',beta)\n",
    "print('DML S.E.:', se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
